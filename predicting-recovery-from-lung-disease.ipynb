{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10845412,"sourceType":"datasetVersion","datasetId":6735523}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction:\n### As a beginner in data science, this notebook serves as part of my learning journey.  \n### In this analysis, I explore binary classification techniques to predict recovery from lung disease using four different models: Logistic Regression, Random Forest, XGBoost and LightGBM.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:02.819151Z","iopub.execute_input":"2025-03-21T09:06:02.819753Z","iopub.status.idle":"2025-03-21T09:06:02.824038Z","shell.execute_reply.started":"2025-03-21T09:06:02.819716Z","shell.execute_reply":"2025-03-21T09:06:02.822937Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Exploratory Data Analysis (EDA) and Visualization\n### In this section, I performed exploratory data analysis to better understand the dataset and visualize important features related to recovery status.\n\n* Recovery Status: A count plot of recovery status distribution.\n* Numerical Features: Overlapping histgrams, with different colors for recovered and non-recovered individuals.\n* Categorical Features: Count plots showing categorical variables, with colors separating recovered and non-recovered individuals.\n\nThe correlations between features were visualized after preprocessing.","metadata":{}},{"cell_type":"code","source":"## Data loading\ndf = pd.read_csv(\"/kaggle/input/lungs-diseases-dataset/lung_disease_data.csv\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:06.960028Z","iopub.execute_input":"2025-03-21T09:06:06.960359Z","iopub.status.idle":"2025-03-21T09:06:07.030113Z","shell.execute_reply.started":"2025-03-21T09:06:06.960332Z","shell.execute_reply":"2025-03-21T09:06:07.029209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:07.324287Z","iopub.execute_input":"2025-03-21T09:06:07.324655Z","iopub.status.idle":"2025-03-21T09:06:07.329901Z","shell.execute_reply.started":"2025-03-21T09:06:07.324612Z","shell.execute_reply":"2025-03-21T09:06:07.328895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:10.469612Z","iopub.execute_input":"2025-03-21T09:06:10.469927Z","iopub.status.idle":"2025-03-21T09:06:10.498748Z","shell.execute_reply.started":"2025-03-21T09:06:10.469902Z","shell.execute_reply":"2025-03-21T09:06:10.497691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:10.784885Z","iopub.execute_input":"2025-03-21T09:06:10.785207Z","iopub.status.idle":"2025-03-21T09:06:10.823861Z","shell.execute_reply.started":"2025-03-21T09:06:10.785182Z","shell.execute_reply":"2025-03-21T09:06:10.822717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Divide columns based on feature type: numerical or categorical\n\ncat_cols = []\nnum_cols = []\ntarget_col = []\n\nfor col in df:\n    if col == 'Recovered':\n        target_col.append(col)\n    \n    elif df[col].dtype == 'object':\n        cat_cols.append(col)\n\n    else:\n        num_cols.append(col)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:13.171746Z","iopub.execute_input":"2025-03-21T09:06:13.172083Z","iopub.status.idle":"2025-03-21T09:06:13.177031Z","shell.execute_reply.started":"2025-03-21T09:06:13.172058Z","shell.execute_reply":"2025-03-21T09:06:13.175835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Count plot of Recovery status\n\nsns.set(font_scale = 0.8, palette='colorblind', style='whitegrid')\n\nplt.figure(figsize = (3,3))\nsns.countplot(df, x = 'Recovered', alpha = 0.7)\nplt.title('Distribution of Recovery Status')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:20.306666Z","iopub.execute_input":"2025-03-21T09:06:20.306990Z","iopub.status.idle":"2025-03-21T09:06:20.636030Z","shell.execute_reply.started":"2025-03-21T09:06:20.306964Z","shell.execute_reply":"2025-03-21T09:06:20.635011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Histograms of Numerical features\n\nfig, axes = plt.subplots(1, 3, figsize = (9, 3))\naxes = axes.ravel()\n\nfor col, ax in zip(num_cols, axes):\n    sns.histplot(df, x=col, hue='Recovered', ax = ax, alpha = 0.7)\n    sns.move_legend(ax, 'lower right')\n    ax.set_title(f\"Distribution of {col}\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:23.275108Z","iopub.execute_input":"2025-03-21T09:06:23.275453Z","iopub.status.idle":"2025-03-21T09:06:24.407881Z","shell.execute_reply.started":"2025-03-21T09:06:23.275428Z","shell.execute_reply":"2025-03-21T09:06:24.406757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Count plots of Categorical features\n\nfig, axes = plt.subplots(2, 2, figsize = (6, 6))\naxes = axes.ravel()\n\nfor col, ax in zip(cat_cols, axes):\n    sns.countplot(df, x = col, hue = 'Recovered', ax = ax, alpha = 0.7)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n    sns.move_legend(ax, 'lower right')\n    ax.set_title(f\"{col} vs Recovery Status\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:27.736826Z","iopub.execute_input":"2025-03-21T09:06:27.737272Z","iopub.status.idle":"2025-03-21T09:06:28.760610Z","shell.execute_reply.started":"2025-03-21T09:06:27.737233Z","shell.execute_reply":"2025-03-21T09:06:28.759381Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Data Preprocessing\n### In this section, I processed the data to prepare it for model training. After that, I visualized the correlations between features.\n## 2.1 Handling Missing Values\n* Numerical features: Missing values were filled with the median of each feature. \n* Categorical features: Missing values were filled with the mode of each feature. \n* Recovery status: Rows with missing recovery status were dropped.","metadata":{}},{"cell_type":"code","source":"## Handling missing values\n## Processed dataset -->> df_im\ndf_im = df.copy()\n\nfor col in num_cols:\n    df_im[col] = df_im[col].fillna(df_im[col].median())\n\nfor col in cat_cols:\n    md = df_im[col].mode()[0]\n    df_im[col] = df_im[col].fillna(md)\n\ndf_im = df_im.dropna(subset = target_col)\nprint(df_im.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:07:39.291470Z","iopub.execute_input":"2025-03-21T09:07:39.291902Z","iopub.status.idle":"2025-03-21T09:07:39.310961Z","shell.execute_reply.started":"2025-03-21T09:07:39.291868Z","shell.execute_reply":"2025-03-21T09:07:39.310070Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.2 Handling Categorical Variables\n* I used Label Encoding to convert categorical variables into numerical values.\n* Specifically, I transformed the recovery status ('Yes'->1, 'No'->0) using the map() function.","metadata":{}},{"cell_type":"code","source":"## Label encoding\n## Processed dataset -->> df_le\n\ndf_le = df_im.copy()\nlabel_encoders = {}\n\nfor col in cat_cols:\n    le = LabelEncoder()\n    df_le[col] = le.fit_transform(df_le[col])\n    label_list = le.classes_\n    print(\"{}: {} -->> {}\".format(col, label_list, [i for i in range(len(label_list))]))\n\n## Convert recovery status\ndf_le['Recovered'] = df_le['Recovered'].map({'Yes':1, 'No':0})\nprint(\"{}: ['No', 'Yes'] -->> [0, 1]\".format(*target_col))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:08:37.706009Z","iopub.execute_input":"2025-03-21T09:08:37.706414Z","iopub.status.idle":"2025-03-21T09:08:37.723023Z","shell.execute_reply.started":"2025-03-21T09:08:37.706386Z","shell.execute_reply":"2025-03-21T09:08:37.722077Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.3 Visualizing Correlations\nI visualized the correlation coefficients between the features to better understand the relationships betwwen them.","metadata":{}},{"cell_type":"code","source":"## Correlation coefficients between features\nplt.figure(figsize=(8,8))\nsns.heatmap(df_le.corr(), square=True, annot=True, fmt=\".2f\", cmap='seismic')\nplt.title(\"Feature Correlation Heatmap\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:10:29.178241Z","iopub.execute_input":"2025-03-21T09:10:29.178686Z","iopub.status.idle":"2025-03-21T09:10:29.652553Z","shell.execute_reply.started":"2025-03-21T09:10:29.178656Z","shell.execute_reply":"2025-03-21T09:10:29.651571Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Model Training & Evaluation\n### In this section, I trained four different models for binary classification: Logistic Regression, Random Forest, XGBoost and LightGBM. \n### I then evaluated their performance using various metrics, including accuracy, precision, recall, F1-score, and AUC.\n### Finally, I visualized the feature importances for each model.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:15:50.882630Z","iopub.execute_input":"2025-03-21T09:15:50.883021Z","iopub.status.idle":"2025-03-21T09:15:55.181163Z","shell.execute_reply.started":"2025-03-21T09:15:50.882994Z","shell.execute_reply":"2025-03-21T09:15:55.179968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Split the dataset into train(80%) and test(20%)\nX = df_le.drop(columns = ['Recovered'])\ny = df_le['Recovered']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 12)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:16:22.032443Z","iopub.execute_input":"2025-03-21T09:16:22.033202Z","iopub.status.idle":"2025-03-21T09:16:22.041322Z","shell.execute_reply.started":"2025-03-21T09:16:22.033168Z","shell.execute_reply":"2025-03-21T09:16:22.040608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Train models\nmodels = {\n    'Logistic Regression': LogisticRegression(),\n    'Random Forest': RandomForestClassifier(n_estimators = 100, random_state = 42),\n    'XGBoost': XGBClassifier(),\n    'LightGBM': LGBMClassifier()\n}\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(f'{name} Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n    print(f'{name} AUC: {roc_auc_score(y_test, y_pred):.4f}')\n    print(classification_report(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:18:14.239236Z","iopub.execute_input":"2025-03-21T09:18:14.239669Z","iopub.status.idle":"2025-03-21T09:18:15.215971Z","shell.execute_reply.started":"2025-03-21T09:18:14.239637Z","shell.execute_reply":"2025-03-21T09:18:15.214853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Plot ROC curve\nplt.figure()\n\nfor name, model in models.items():\n    fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label = f'{name} (AUC = {roc_auc:.2f})')\n\nplt.plot([0, 1], [0, 1], linestyle = '--', color = 'gray')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.legend(loc = 'lower right')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:19:28.029569Z","iopub.execute_input":"2025-03-21T09:19:28.030057Z","iopub.status.idle":"2025-03-21T09:19:28.367092Z","shell.execute_reply.started":"2025-03-21T09:19:28.030024Z","shell.execute_reply":"2025-03-21T09:19:28.366089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Visualizing feature importance\ndt_models = models.copy()\ndel dt_models['Logistic Regression']\n\nfig, axes = plt.subplots(1, 3, figsize = (15, 5))\naxes = axes.ravel()\n\nfor name, model, ax in zip(dt_models.keys(), dt_models.values(), axes):\n\n    feature_importance = model.feature_importances_\n    features = X_train.columns\n    importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importance})\n    ax.barh(importance_df['Feature'], importance_df['Importance'])\n    ax.set_xlabel('Importance')\n    ax.set_ylabel('Feature')\n    ax.set_title(f'Feature Importance of {name} Model')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:21:31.718267Z","iopub.execute_input":"2025-03-21T09:21:31.718730Z","iopub.status.idle":"2025-03-21T09:21:32.695502Z","shell.execute_reply.started":"2025-03-21T09:21:31.718695Z","shell.execute_reply":"2025-03-21T09:21:32.694680Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion:\n* These results suggest that the current features may not fully capture the patterns needed for accurate recovery prediction, as indicated by the low model performance and weak correlations among features.\n* Including more relevant factors, such as biochemical markers, genetic data, medication type, severity, symptoms, complications, and length of treatment, could improve prediction.\n* While tree-based models were tested, alternative approaches, such as deep learning, might be worth exploring.  \nThis project was a valuable learning experience, and I appreciate the opportunity to work with this dataset.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}